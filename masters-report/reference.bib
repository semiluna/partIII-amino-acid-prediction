@article{hunter1993molecular,
  title={Molecular biology for computer scientists},
  author={Hunter, Lawrence},
  journal={Artificial intelligence and molecular biology},
  volume={177},
  pages={1--46},
  year={1993},
  publisher={Aaai Press Menlo Park}
}

@misc{relu,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@Article{Lu2022,
author={Lu, Hongyuan
and Diaz, Daniel J.
and Czarnecki, Natalie J.
and Zhu, Congzhi
and Kim, Wantae
and Shroff, Raghav
and Acosta, Daniel J.
and Alexander, Bradley R.
and Cole, Hannah O.
and Zhang, Yan
and Lynd, Nathaniel A.
and Ellington, Andrew D.
and Alper, Hal S.},
title={Machine learning-aided engineering of hydrolases for PET depolymerization},
journal={Nature},
year={2022},
month={Apr},
day={01},
volume={604},
number={7907},
pages={662-667},
abstract={Plastic waste poses an ecological challenge1--3 and enzymatic degradation offers one, potentially green and scalable, route for polyesters waste recycling4. Poly(ethylene terephthalate) (PET) accounts for 12{\%} of global solid waste5, and a circular carbon economy for PET is theoretically attainable through rapid enzymatic depolymerization followed by repolymerization or conversion/valorization into other products6--10. Application of PET hydrolases, however, has been hampered by their lack of robustness to pH and temperature ranges, slow reaction rates and inability to directly use untreated postconsumer plastics11. Here, we use a structure-based, machine learning algorithm to engineer a robust and active PET hydrolase. Our mutant and scaffold combination (FAST-PETase: functional, active, stable and tolerant PETase) contains five mutations compared to wild-type PETase (N233K/R224Q/S121E from prediction and D186H/R280A from scaffold) and shows superior PET-hydrolytic activity relative to both wild-type and engineered alternatives12 between 30 and 50{\thinspace}{\textdegree}C and a range of pH levels. We demonstrate that untreated, postconsumer-PET from 51 different thermoformed products can all be almost completely degraded by FAST-PETase in 1{\thinspace}week. FAST-PETase can also depolymerize untreated, amorphous portions of a commercial water bottle and an entire thermally pretreated water bottle at 50{\thinspace}{\textordmasculine}C. Finally, we demonstrate a closed-loop PET recycling process by using FAST-PETase and resynthesizing PET from the recovered monomers. Collectively, our results demonstrate a viable route for enzymatic plastic recycling at the industrial scale.},
issn={1476-4687},
doi={10.1038/s41586-022-04599-z},
url={https://doi.org/10.1038/s41586-022-04599-z}
}

@article{Henikoff1992,
  author = {Henikoff, S and Henikoff, J G},
  title = {Amino Acid Substitution Matrices from Protein Blocks},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {89},
  number = {22},
  pages = {10915--10919},
  year = {1992},
  month = {Nov},
  doi = {10.1073/pnas.89.22.10915},
  issn = {0027-8424},
  eissn = {1091-6490},
  pmid = {1438297},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC50453/},
  abstract = {Methods for alignment of protein sequences typically measure similarity by using a substitution matrix with scores for all possible exchanges of one amino acid with another. The most widely used matrices are based on the Dayhoff model of evolutionary rates. Using a different approach, we have derived substitution matrices from about 2000 blocks of aligned sequence segments characterizing more than 500 groups of related proteins. This led to marked improvements in alignments and in searches using queries from each of the groups.},
  keywords = {Algorithms, Amino Acid Sequence, Animals, Caenorhabditis elegans/genetics, Drosophila/genetics, Lod Score, Mathematics, Molecular Sequence Data, Probability, Proteins/chemistry/genetics, Sequence Homology, Amino Acid, Software},
  language = {eng},
  note = {Comparative Study, Journal Article, Research Support, U.S. Gov't, P.H.S.},
  affiliation = {Howard Hughes Medical Institute, Fred Hutchinson Cancer Research Center, Seattle, WA 98104, United States},
}

@misc{bepler2019learning,
      title={Learning protein sequence embeddings using information from structure}, 
      author={Tristan Bepler and Bonnie Berger},
      year={2019},
      eprint={1902.08661},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lstms,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}





@article{alley2019unified,
  title={Unified rational protein engineering with sequence-based deep representation learning},
  author={Alley, Eric C and Khimulya, Grigory and Biswas, Sushovan and others},
  journal={Nature Methods},
  volume={16},
  number={12},
  pages={1315--1322},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41592-019-0598-1}
}

@article{heinzinger2019modeling,
  title={Modeling aspects of the language of life through transfer-learning protein sequences},
  author={Heinzinger, Michael and Elnaggar, Ahmed and Wang, Yan and others},
  journal={BMC Bioinformatics},
  volume={20},
  number={1},
  pages={723},
  year={2019},
  publisher={BioMed Central},
  doi={10.1186/s12859-019-3220-8}
}



@article{deepsequence,
	author = {Adam J. Riesselman and John B. Ingraham and Debora S. Marks},
	title = {Deep generative models of genetic variation capture mutation effects},
	elocation-id = {235655},
	year = {2017},
	doi = {10.1101/235655},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The functions of proteins and RNAs are determined by a myriad of interactions between their constituent residues, but most quantitative models of how molecular phenotype depends on genotype must approximate this by simple additive effects. While recent models have relaxed this constraint to also account for pairwise interactions, these approaches do not provide a tractable path towards modeling higher-order dependencies. Here, we show how latent variable models with nonlinear dependencies can be applied to capture beyond-pairwise constraints in biomolecules. We present a new probabilistic model for sequence families, DeepSequence, that can predict the effects of mutations across a variety of deep mutational scanning experiments significantly better than site independent or pairwise models that are based on the same evolutionary data. The model, learned in an unsupervised manner solely from sequence information, is grounded with biologically motivated priors, reveals latent organization of sequence families, and can be used to extrapolate to new parts of sequence space.},
	URL = {https://www.biorxiv.org/content/early/2017/12/18/235655.1},
	eprint = {https://www.biorxiv.org/content/early/2017/12/18/235655.1.full.pdf},
	journal = {bioRxiv}
}

@article{EVE,
  title = {Disease variant prediction with deep generative models of evolutionary data},
  author = {Frazer, Jonathan and Notin, Pascal and Dias, Mafalda and Gomez, Aidan and Min, Joseph K. and Brock, Kelly and Gal, Yarin and Marks, Debora S.},
  journal = {Nature},
  volume = {599},
  number = {7883},
  pages = {91--95},
  year = {2021},
  month = {November},
  doi = {10.1038/s41586-021-04043-8},
  url = {https://doi.org/10.1038/s41586-021-04043-8},
  abstract = {Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98\%) of these variants still have unknown consequences1–3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4–10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification12–16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.},
  issn = {1476-4687},
  publisher = {Nature Publishing Group},
}



@inproceedings{eqgat2,
title={Representation Learning on Biomolecular Structures using Equivariant Graph Attention},
author={Tuan Le and Frank Noe and Djork-Arn{\'e} Clevert},
booktitle={Learning on Graphs Conference},
year={2022},
url={https://openreview.net/forum?id=kv4xUo5Pu6}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@misc{gelu,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2020},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{silu,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{aa-index,
  title = "AAindex: Amino Acid Index Database",
  author = "Kawashima, S and Ogata, H and Kanehisa, M",
  year = "1999",
  month = "Jan",
  volume = "27",
  issue = "1",
  pages = "368-9",
  journal = "Nucleic Acids Res",
  doi = "10.1093/nar/27.1.368",
  url = "https://doi.org/10.1093/nar/27.1.368",
  publisher = "Oxford University Press",
}


@article{rcsb_pdb,
  author = {Berman, H.M. and Westbrook, J. and Feng, Z. and Gilliland, G. and Bhat, T.N. and Weissig, H. and Shindyalov, I.N. and Bourne, P.E.},
  title = {The Protein Data Bank (2000)},
  journal = {Nucleic Acids Research},
  volume = {28},
  number = {1},
  pages = {235--242},
  year = {2000},
  url = {https://doi.org/10.1093/nar/28.1.235},
}

@article{chloe-hsu,
	author = {Chloe Hsu and Hunter Nisonoff and Clara Fannjiang and Jennifer Listgarten},
	title = {Combining evolutionary and assay-labelled data for protein fitness prediction},
	elocation-id = {2021.03.28.437402},
	year = {2021},
	doi = {10.1101/2021.03.28.437402},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Predictive modelling of protein properties has become increasingly important to the field of machine-learning guided protein engineering. In one of the two existing approaches, evolutionarily-related sequences to a query protein drive the modelling process, without any property measurements from the laboratory. In the other, a set of protein variants of interest are assayed, and then a supervised regression model is estimated with the assay-labelled data. Although a handful of recent methods have shown promise in combining the evolutionary and supervised approaches, this hybrid problem has not been examined in depth, leaving it unclear how practitioners should proceed, and how method developers should build on existing work. Herein, we present a systematic assessment of methods for protein fitness prediction when evolutionary and assay-labelled data are available. We find that a simple baseline approach we introduce is competitive with and often outperforms more sophisticated methods. Moreover, our simple baseline is plug-and-play with a wide variety of established methods, and does not add any substantial computational burden. Our analysis highlights the importance of systematic evaluations and sufficient baselines.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2021/03/29/2021.03.28.437402},
	eprint = {https://www.biorxiv.org/content/early/2021/03/29/2021.03.28.437402.full.pdf},
	journal = {bioRxiv}
}

@article{prottrans,
  title = {ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning},
  author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {10},
  pages = {7112--7127},
  year = {2022},
  month = {October},
  doi = {10.1109/TPAMI.2021.3095381},
  url = {https://doi.org/10.1109/TPAMI.2021.3095381},
  abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81\%-87\%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81\%) and membrane versus water-soluble (2-state accuracy Q2=91\%). For secondary structure, the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the grammar of the language of life. All our models are available through https://github.com/agemagician/ProtTrans.},
  issn = {1939-3539},
  issn-linking = {0098-5589},
  keywords = {Algorithms, Computational Biology/methods, Natural Language Processing, Proteins/chemistry, Supervised Machine Learning},
  language = {eng},
  publisher = {IEEE},
}

@software{pymol,
  author = {Schrödinger, LLC and Warren DeLano},
  title = {PyMOL},
  url = {http://www.pymol.org/pymol},
  version = {2.4.0},
  date = {2020-05-20},
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pyg,
  doi = {10.48550/ARXIV.1903.02428},
  
  url = {https://arxiv.org/abs/1903.02428},
  
  author = {Fey, Matthias and Lenssen, Jan Eric},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Fast Graph Representation Learning with PyTorch Geometric},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ESM-1v,
  title={Language models enable zero-shot prediction of the effects of mutations on protein function},
  author={Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29287--29303},
  year={2021}
}

@misc{gearnet,
      title={Protein Representation Learning by Geometric Structure Pretraining}, 
      author={Zuobai Zhang and Minghao Xu and Arian Jamasb and Vijil Chenthamarakshan and Aurelie Lozano and Payel Das and Jian Tang},
      year={2023},
      eprint={2203.06125},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{ESM,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{Dawson2017CATH,
  author = {Dawson, Natalie L. and Lewis, Tony E. and Das, Sayoni and Lees, Jonathan G. and Lee, David and Ashford, Paul and Orengo, Christine A. and Sillitoe, Ian},
  title = {{CATH: An Expanded Resource to Predict Protein Function Through Structure and Sequence}},
  journal = {Nucleic Acids Res},
  volume = {45},
  number = {D1},
  pages = {D289--D295},
  year = {2017},
  month = {Jan},
  day = {4},
  doi = {10.1093/nar/gkw1098},
  url = {https://doi.org/10.1093/nar/gkw1098},
  eprint = {https://academic.oup.com/nar/article-pdf/45/D1/D289/18419876/gkw1098.pdf},
  issn = {1362-4962},
  pmid = {27899584},
  pmcid = {PMC5210570},
}


@inproceedings{schutt2021equivariant,
  title={Equivariant message passing for the prediction of tensorial properties and molecular spectra},
  author={Sch{\"u}tt, Kristof and Unke, Oliver and Gastegger, Michael},
  booktitle={International Conference on Machine Learning},
  pages={9377--9388},
  year={2021},
  organization={PMLR}
}

@article{egnn-application-1,
    author = {Kim, Ha Young and Kim, Sungsik and Park, Woong-Yang and Kim, Dongsup},
    title = "{G-RANK: an equivariant graph neural network for the scoring of protein–protein docking models}",
    journal = {Bioinformatics Advances},
    volume = {3},
    number = {1},
    year = {2023},
    month = {02},
    abstract = "{Protein complex structure prediction is important for many applications in bioengineering. A widely used method for predicting the structure of protein complexes is computational docking. Although many tools for scoring protein–protein docking models have been developed, it is still a challenge to accurately identify near-native models for unknown protein complexes. A recently proposed model called the geometric vector perceptron–graph neural network (GVP-GNN), a subtype of equivariant graph neural networks, has demonstrated success in various 3D molecular structure modeling tasks.Herein, we present G-RANK, a GVP-GNN-based method for the scoring of protein-protein docking models. When evaluated on two different test datasets, G-RANK achieved a performance competitive with or better than the state-of-the-art scoring functions. We expect G-RANK to be a useful tool for various applications in biological engineering.Source code is available at https://github.com/ha01994/grank.kds@kaist.ac.krSupplementary data are available at Bioinformatics Advances online.}",
    issn = {2635-0041},
    doi = {10.1093/bioadv/vbad011},
    url = {https://doi.org/10.1093/bioadv/vbad011},
    note = {vbad011},
    eprint = {https://academic.oup.com/bioinformaticsadvances/article-pdf/3/1/vbad011/49178624/vbad011.pdf},
}





@article{alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group},
  doi={10.1038/s41586-021-03819-2},
  url={https://doi.org/10.1038/s41586-021-03819-2},
  issn={1476-4687},
  abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
}


@article{insulin,
  author = {Brange, J. and Ribel, U. and Hansen, J. F. and Dodson, G. and Hansen, M. T. and Havelund, S. and Melberg, S. G. and Norris, F. and Norris, K. and Snel, L. and Sørensen, A. R. and Voigt, H. O.},
  title = {Monomeric insulins obtained by protein engineering and their medical implications},
  journal = {Nature},
  volume = {333},
  number = {6174},
  pages = {679--682},
  year = {1988},
  month = {06},
  doi = {10.1038/333679a0},
  url = {https://doi.org/10.1038/333679a0},
  abstract = {The use of insulin as an injected therapeutic agent for the treatment of diabetes has been one of the outstanding successes of modern medicine. The therapy has, however, had its associated problems, not least because injection of insulin does not lead to normal diurnal concentrations of insulin in the blood. This is especially true at meal times when absorption from subcutaneous tissue is too slow to mimic the normal rapid increments of insulin in the blood. In the neutral solutions used for therapy, insulin is mostly assembled as zinc-containing hexamers1 and this self-association, which under normal physiological circumstances functions to facilitate proinsulin transport, conversion and intracellular storage2, may limit the rate of absorption. We now report that it is possible, by single amino-acid substitutions, to make insulins which are essentially monomeric at pharmaceutical concentrations (0.6 mM) and which have largely preserved their biological activity. These monomeric insulins are absorbed two to three times faster after subcutaneous injection than the present rapid-acting insulins. They are therefore capable of giving diabetic patients a more physiological plasma insulin profile at the time of meal consumption.},
  issn = {1476-4687},
}


@misc{layernorm,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{eqgat,
      title={Equivariant Graph Attention Networks for Molecular Property Prediction}, 
      author={Tuan Le and Frank Noé and Djork-Arné Clevert},
      year={2022},
      eprint={2202.09891},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gat,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@article{madani2020progen,
  title={Progen: Language modeling for protein generation},
  author={Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R and Huang, Po-Ssu and Socher, Richard},
  journal={arXiv preprint arXiv:2004.03497},
  year={2020}
}

@misc{elnaggar2021prottrans,
      title={ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing}, 
      author={Ahmed Elnaggar and Michael Heinzinger and Christian Dallago and Ghalia Rihawi and Yu Wang and Llion Jones and Tom Gibbs and Tamas Feher and Christoph Angerer and Martin Steinegger and Debsindhu Bhowmik and Burkhard Rost},
      year={2021},
      eprint={2007.06225},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@Article{Lu2022,
author={Lu, Hongyuan
and Diaz, Daniel J.
and Czarnecki, Natalie J.
and Zhu, Congzhi
and Kim, Wantae
and Shroff, Raghav
and Acosta, Daniel J.
and Alexander, Bradley R.
and Cole, Hannah O.
and Zhang, Yan
and Lynd, Nathaniel A.
and Ellington, Andrew D.
and Alper, Hal S.},
title={Machine learning-aided engineering of hydrolases for PET depolymerization},
journal={Nature},
year={2022},
month={Apr},
day={01},
volume={604},
number={7907},
pages={662-667},
abstract={Plastic waste poses an ecological challenge1--3 and enzymatic degradation offers one, potentially green and scalable, route for polyesters waste recycling4. Poly(ethylene terephthalate) (PET) accounts for 12{\%} of global solid waste5, and a circular carbon economy for PET is theoretically attainable through rapid enzymatic depolymerization followed by repolymerization or conversion/valorization into other products6--10. Application of PET hydrolases, however, has been hampered by their lack of robustness to pH and temperature ranges, slow reaction rates and inability to directly use untreated postconsumer plastics11. Here, we use a structure-based, machine learning algorithm to engineer a robust and active PET hydrolase. Our mutant and scaffold combination (FAST-PETase: functional, active, stable and tolerant PETase) contains five mutations compared to wild-type PETase (N233K/R224Q/S121E from prediction and D186H/R280A from scaffold) and shows superior PET-hydrolytic activity relative to both wild-type and engineered alternatives12 between 30 and 50{\thinspace}{\textdegree}C and a range of pH levels. We demonstrate that untreated, postconsumer-PET from 51 different thermoformed products can all be almost completely degraded by FAST-PETase in 1{\thinspace}week. FAST-PETase can also depolymerize untreated, amorphous portions of a commercial water bottle and an entire thermally pretreated water bottle at 50{\thinspace}{\textordmasculine}C. Finally, we demonstrate a closed-loop PET recycling process by using FAST-PETase and resynthesizing PET from the recovered monomers. Collectively, our results demonstrate a viable route for enzymatic plastic recycling at the industrial scale.},
issn={1476-4687},
doi={10.1038/s41586-022-04599-z},
url={https://doi.org/10.1038/s41586-022-04599-z}
}

@inproceedings{satorras2021n,
  title={E (n) equivariant graph neural networks},
  author={Satorras, V{\i}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  booktitle={International conference on machine learning},
  pages={9323--9332},
  year={2021},
  organization={PMLR}
}

@misc{liao2023equiformer,
      title={Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs}, 
      author={Yi-Lun Liao and Tess Smidt},
      year={2023},
      eprint={2206.11990},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{mace,
 author = {Batatia, Ilyes and Kovacs, David P and Simm, Gregor and Ortner, Christoph and Csanyi, Gabor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {11423--11436},
 publisher = {Curran Associates, Inc.},
 title = {MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4a36c3c51af11ed9f34615b81edb5bbc-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{finzi2020generalizing,
  title={Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data},
  author={Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  pages={3165--3176},
  year={2020},
  organization={PMLR}
}

@article{fuchs2020se,
  title={Se (3)-transformers: 3d roto-translation equivariant attention networks},
  author={Fuchs, Fabian and Worrall, Daniel and Fischer, Volker and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1970--1981},
  year={2020}
}

@article{batatia2022design,
  title={The design space of e (3)-equivariant atom-centered interatomic potentials},
  author={Batatia, Ilyes and Batzner, Simon and Kov{\'a}cs, D{\'a}vid P{\'e}ter and Musaelian, Albert and Simm, Gregor NC and Drautz, Ralf and Ortner, Christoph and Kozinsky, Boris and Cs{\'a}nyi, G{\'a}bor},
  journal={arXiv preprint arXiv:2205.06643},
  year={2022}
}

@article{anderson2019cormorant,
  title={Cormorant: Covariant molecular neural networks},
  author={Anderson, Brandon and Hy, Truong Son and Kondor, Risi},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{thomas2018tensor,
      title={Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds}, 
      author={Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley},
      year={2018},
      eprint={1802.08219},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gcn,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{torng20173d,
  title={3D deep convolutional neural networks for amino acid environment similarity analysis},
  author={Torng, Wenzhi and Altman, Russ B.},
  journal={BMC Bioinformatics},
  volume={18},
  number={1},
  pages={302},
  year={2017},
  publisher={Springer},
  doi={10.1186/s12859-017-1702-0},
  url={https://doi.org/10.1186/s12859-017-1702-0},
}


@article{mutcompute,
  title={Discovery of Novel Gain-of-Function Mutations Guided by Structure-Based Deep Learning},
  author={Shroff, Raghav and Cole, Austin W. and Diaz, Daniel J. and Morrow, Barrett R. and Donnell, Isaac and Annapareddy, Ankur and Gollihar, Jimmy and Ellington, Andrew D. and Thyer, Ross},
  journal={ACS Synthetic Biology},
  volume={9},
  number={11},
  pages={2927--2935},
  year={2020},
  publisher={American Chemical Society},
  doi={10.1021/acssynbio.0c00345},
  url={https://doi.org/10.1021/acssynbio.0c00345},
}


@article{portapardo2022structural,
  title={The structural coverage of the human proteome before and after AlphaFold},
  author={Porta-Pardo, Eduard and Ruiz-Serra, Victor and Valentini, Simone and Valencia, Alfonso},
  journal={PLoS computational biology},
  volume={18},
  number={1},
  pages={e1009818},
  year={2022},
  month={Jan},
  day={24},
  doi={10.1371/journal.pcbi.1009818},
  PMID={35073311},
  PMCID={PMC8812986}
}

@article{uniref,
  title={UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches},
  author={Suzek, Baris E and Wang, Yongxing and Huang, Haixu and McGarvey, Peter B and Wu, Cathy H and UniProt Consortium},
  journal={Bioinformatics},
  volume={31},
  number={6},
  pages={926--932},
  year={2015},
  month={Mar},
  doi={10.1093/bioinformatics/btu739},
  publisher={Oxford University Press},
  issn={1367-4803},
  pmid={25398609},
  pmcid={PMC4375400},
  url={https://doi.org/10.1093/bioinformatics/btu739}
}


@article{vig2020bertology,
  title={BERTology meets biology: interpreting attention in protein language models},
  author={Vig, Jesse and Madani, Ali and Varshney, Lav R and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv preprint arXiv:2006.15222},
  year={2020}
}



@article{meier2021language,
  title={Language models enable zero-shot prediction of the effects of mutations on protein function},
  author={Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29287--29303},
  year={2021}
}

@article {Rao2020,
	author = {Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},
	title = {Transformer protein language models are unsupervised structure learners},
	elocation-id = {2020.12.15.422761},
	year = {2020},
	doi = {10.1101/2020.12.15.422761},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.1Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2020/12/15/2020.12.15.422761},
	eprint = {https://www.biorxiv.org/content/early/2020/12/15/2020.12.15.422761.full.pdf},
	journal = {bioRxiv}
}


@inproceedings{nambiar2020transforming,
  title={Transforming the language of life: transformer neural networks for protein prediction tasks},
  author={Nambiar, Ananthan and Heflin, Maeve and Liu, Simon and Maslov, Sergei and Hopkins, Mark and Ritz, Anna},
  booktitle={Proceedings of the 11th ACM international conference on bioinformatics, computational biology and health informatics},
  pages={1--8},
  year={2020}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@misc{gvp1,
      title={Learning from Protein Structure with Geometric Vector Perceptrons}, 
      author={Bowen Jing and Stephan Eismann and Patricia Suriana and Raphael J. L. Townshend and Ron Dror},
      year={2021},
      eprint={2009.01411},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM}
}

@article{gvp2,
  author       = {Bowen Jing and
                  Stephan Eismann and
                  Pratham N. Soni and
                  Ron O. Dror},
  title        = {Equivariant Graph Neural Networks for 3D Macromolecular Structure},
  journal      = {CoRR},
  volume       = {abs/2106.03843},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.03843},
  eprinttype    = {arXiv},
  eprint       = {2106.03843},
  timestamp    = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-03843.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{tranception,
  title = 	 {Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval},
  author =       {Notin, Pascal and Dias, Mafalda and Frazer, Jonathan and Hurtado, Javier Marchena and Gomez, Aidan N and Marks, Debora and Gal, Yarin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16990--17017},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/notin22a/notin22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/notin22a.html},
  abstract = 	 {The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym – an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.}
}


@article{Lu2022,
author = {Lu, Hongyuan and Diaz, Daniel J. and Czarnecki, Natalie J. and Zhu, Congzhi and Kim, Wantae and Shroff, Raghav and Acosta, Daniel J. and Alexander, Bradley R. and Cole, Hannah O. and Zhang, Yan and Lynd, Nathaniel A. and Ellington, Andrew D. and Alper, Hal S.},
title = {Machine learning-aided engineering of hydrolases for PET depolymerization},
journal = {Nature},
year = {2022},
volume = {604},
number = {7907},
pages = {662--667},
month = apr,
abstract = {Plastic waste poses an ecological challenge1-3 and enzymatic degradation offers one, potentially green and scalable, route for polyesters waste recycling4. Poly(ethylene terephthalate) (PET) accounts for 12\% of global solid waste5, and a circular carbon economy for PET is theoretically attainable through rapid enzymatic depolymerization followed by repolymerization or conversion/valorization into other products6–10. Application of PET hydrolases, however, has been hampered by their lack of robustness to pH and temperature ranges, slow reaction rates and inability to directly use untreated postconsumer plastics11. Here, we use a structure-based, machine learning algorithm to engineer a robust and active PET hydrolase. Our mutant and scaffold combination (FAST-PETase: functional, active, stable and tolerant PETase) contains five mutations compared to wild-type PETase (N233K/R224Q/S121E from prediction and D186H/R280A from scaffold) and shows superior PET-hydrolytic activity relative to both wild-type and engineered alternatives12 between 30 and 50 °C and a range of pH levels. We demonstrate that untreated, postconsumer-PET from 51 different thermoformed products can all be almost completely degraded by FAST-PETase in 1 week. FAST-PETase can also depolymerize untreated, amorphous portions of a commercial water bottle and an entire thermally pretreated water bottle at 50 ºC. Finally, we demonstrate a closed-loop PET recycling process by using FAST-PETase and resynthesizing PET from the recovered monomers. Collectively, our results demonstrate a viable route for enzymatic plastic recycling at the industrial scale.},
issn = {1476-4687},
doi = {10.1038/s41586-022-04599-z},
url = {https://doi.org/10.1038/s41586-022-04599-z},
}

@article{rosenblatt1958,
    author = "Rosenblatt, F.",
    title = "The Perceptron: A Probabilistic Model For Information Storage And Organization in the Brain",
    year = "1958",
    journal = "Psychological Review"
}

@article{backpropagation1,
    author = "Kelley, Henry J.",
    year = "1960",
    title = "Gradient theory of optimal flight paths",
    journal = "ARS Journal. 30 (10): 947-954",
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}


@article{atom-3d,
  author    = {Raphael J. L. Townshend and
               Martin V{\"{o}}gele and
               Patricia Suriana and
               Alexander Derry and
               Alexander Powers and
               Yianni Laloudakis and
               Sidhika Balachandar and
               Brandon M. Anderson and
               Stephan Eismann and
               Risi Kondor and
               Russ B. Altman and
               Ron O. Dror},
  title     = {{ATOM3D:} Tasks On Molecules in Three Dimensions},
  journal   = {CoRR},
  volume    = {abs/2012.04035},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.04035},
  eprinttype = {arXiv},
  eprint    = {2012.04035},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-04035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}