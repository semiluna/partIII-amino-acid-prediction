@article{hunter1993molecular,
  title={Molecular biology for computer scientists},
  author={Hunter, Lawrence},
  journal={Artificial intelligence and molecular biology},
  volume={177},
  pages={1--46},
  year={1993},
  publisher={Aaai Press Menlo Park}
}

@misc{relu,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@misc{gelu,
      title={Gaussian Error Linear Units (GELUs)}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2020},
      eprint={1606.08415},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{silu,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{aa-index,
  title = "AAindex: Amino Acid Index Database",
  author = "Kawashima, S and Ogata, H and Kanehisa, M",
  year = "1999",
  month = "Jan",
  volume = "27",
  issue = "1",
  pages = "368-9",
  journal = "Nucleic Acids Res",
  doi = "10.1093/nar/27.1.368",
  url = "https://doi.org/10.1093/nar/27.1.368",
  publisher = "Oxford University Press",
}


@article{rcsb_pdb,
  author = {Berman, H.M. and Westbrook, J. and Feng, Z. and Gilliland, G. and Bhat, T.N. and Weissig, H. and Shindyalov, I.N. and Bourne, P.E.},
  title = {The Protein Data Bank (2000)},
  journal = {Nucleic Acids Research},
  volume = {28},
  number = {1},
  pages = {235--242},
  year = {2000},
  url = {https://doi.org/10.1093/nar/28.1.235},
}

@article{chloe-hsu,
	author = {Chloe Hsu and Hunter Nisonoff and Clara Fannjiang and Jennifer Listgarten},
	title = {Combining evolutionary and assay-labelled data for protein fitness prediction},
	elocation-id = {2021.03.28.437402},
	year = {2021},
	doi = {10.1101/2021.03.28.437402},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Predictive modelling of protein properties has become increasingly important to the field of machine-learning guided protein engineering. In one of the two existing approaches, evolutionarily-related sequences to a query protein drive the modelling process, without any property measurements from the laboratory. In the other, a set of protein variants of interest are assayed, and then a supervised regression model is estimated with the assay-labelled data. Although a handful of recent methods have shown promise in combining the evolutionary and supervised approaches, this hybrid problem has not been examined in depth, leaving it unclear how practitioners should proceed, and how method developers should build on existing work. Herein, we present a systematic assessment of methods for protein fitness prediction when evolutionary and assay-labelled data are available. We find that a simple baseline approach we introduce is competitive with and often outperforms more sophisticated methods. Moreover, our simple baseline is plug-and-play with a wide variety of established methods, and does not add any substantial computational burden. Our analysis highlights the importance of systematic evaluations and sufficient baselines.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2021/03/29/2021.03.28.437402},
	eprint = {https://www.biorxiv.org/content/early/2021/03/29/2021.03.28.437402.full.pdf},
	journal = {bioRxiv}
}

@article{egnn-application-1,
    author = {Kim, Ha Young and Kim, Sungsik and Park, Woong-Yang and Kim, Dongsup},
    title = "{G-RANK: an equivariant graph neural network for the scoring of protein–protein docking models}",
    journal = {Bioinformatics Advances},
    volume = {3},
    number = {1},
    year = {2023},
    month = {02},
    abstract = "{Protein complex structure prediction is important for many applications in bioengineering. A widely used method for predicting the structure of protein complexes is computational docking. Although many tools for scoring protein–protein docking models have been developed, it is still a challenge to accurately identify near-native models for unknown protein complexes. A recently proposed model called the geometric vector perceptron–graph neural network (GVP-GNN), a subtype of equivariant graph neural networks, has demonstrated success in various 3D molecular structure modeling tasks.Herein, we present G-RANK, a GVP-GNN-based method for the scoring of protein-protein docking models. When evaluated on two different test datasets, G-RANK achieved a performance competitive with or better than the state-of-the-art scoring functions. We expect G-RANK to be a useful tool for various applications in biological engineering.Source code is available at https://github.com/ha01994/grank.kds@kaist.ac.krSupplementary data are available at Bioinformatics Advances online.}",
    issn = {2635-0041},
    doi = {10.1093/bioadv/vbad011},
    url = {https://doi.org/10.1093/bioadv/vbad011},
    note = {vbad011},
    eprint = {https://academic.oup.com/bioinformaticsadvances/article-pdf/3/1/vbad011/49178624/vbad011.pdf},
}





@article{alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group},
  doi={10.1038/s41586-021-03819-2},
  url={https://doi.org/10.1038/s41586-021-03819-2},
  issn={1476-4687},
  abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
}


@article{insulin,
  author = {Brange, J. and Ribel, U. and Hansen, J. F. and Dodson, G. and Hansen, M. T. and Havelund, S. and Melberg, S. G. and Norris, F. and Norris, K. and Snel, L. and Sørensen, A. R. and Voigt, H. O.},
  title = {Monomeric insulins obtained by protein engineering and their medical implications},
  journal = {Nature},
  volume = {333},
  number = {6174},
  pages = {679--682},
  year = {1988},
  month = {06},
  doi = {10.1038/333679a0},
  url = {https://doi.org/10.1038/333679a0},
  abstract = {The use of insulin as an injected therapeutic agent for the treatment of diabetes has been one of the outstanding successes of modern medicine. The therapy has, however, had its associated problems, not least because injection of insulin does not lead to normal diurnal concentrations of insulin in the blood. This is especially true at meal times when absorption from subcutaneous tissue is too slow to mimic the normal rapid increments of insulin in the blood. In the neutral solutions used for therapy, insulin is mostly assembled as zinc-containing hexamers1 and this self-association, which under normal physiological circumstances functions to facilitate proinsulin transport, conversion and intracellular storage2, may limit the rate of absorption. We now report that it is possible, by single amino-acid substitutions, to make insulins which are essentially monomeric at pharmaceutical concentrations (0.6 mM) and which have largely preserved their biological activity. These monomeric insulins are absorbed two to three times faster after subcutaneous injection than the present rapid-acting insulins. They are therefore capable of giving diabetic patients a more physiological plasma insulin profile at the time of meal consumption.},
  issn = {1476-4687},
}


@misc{layernorm,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{eqgat,
      title={Equivariant Graph Attention Networks for Molecular Property Prediction}, 
      author={Tuan Le and Frank Noé and Djork-Arné Clevert},
      year={2022},
      eprint={2202.09891},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{gat,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@misc{gvp1,
      title={Learning from Protein Structure with Geometric Vector Perceptrons}, 
      author={Bowen Jing and Stephan Eismann and Patricia Suriana and Raphael J. L. Townshend and Ron Dror},
      year={2021},
      eprint={2009.01411},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM}
}

@article{gvp2,
  author       = {Bowen Jing and
                  Stephan Eismann and
                  Pratham N. Soni and
                  Ron O. Dror},
  title        = {Equivariant Graph Neural Networks for 3D Macromolecular Structure},
  journal      = {CoRR},
  volume       = {abs/2106.03843},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.03843},
  eprinttype    = {arXiv},
  eprint       = {2106.03843},
  timestamp    = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-03843.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{tranception,
  title = 	 {Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval},
  author =       {Notin, Pascal and Dias, Mafalda and Frazer, Jonathan and Hurtado, Javier Marchena and Gomez, Aidan N and Marks, Debora and Gal, Yarin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16990--17017},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/notin22a/notin22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/notin22a.html},
  abstract = 	 {The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym – an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.}
}


@article{Lu2022,
author = {Lu, Hongyuan and Diaz, Daniel J. and Czarnecki, Natalie J. and Zhu, Congzhi and Kim, Wantae and Shroff, Raghav and Acosta, Daniel J. and Alexander, Bradley R. and Cole, Hannah O. and Zhang, Yan and Lynd, Nathaniel A. and Ellington, Andrew D. and Alper, Hal S.},
title = {Machine learning-aided engineering of hydrolases for PET depolymerization},
journal = {Nature},
year = {2022},
volume = {604},
number = {7907},
pages = {662--667},
month = apr,
abstract = {Plastic waste poses an ecological challenge1-3 and enzymatic degradation offers one, potentially green and scalable, route for polyesters waste recycling4. Poly(ethylene terephthalate) (PET) accounts for 12\% of global solid waste5, and a circular carbon economy for PET is theoretically attainable through rapid enzymatic depolymerization followed by repolymerization or conversion/valorization into other products6–10. Application of PET hydrolases, however, has been hampered by their lack of robustness to pH and temperature ranges, slow reaction rates and inability to directly use untreated postconsumer plastics11. Here, we use a structure-based, machine learning algorithm to engineer a robust and active PET hydrolase. Our mutant and scaffold combination (FAST-PETase: functional, active, stable and tolerant PETase) contains five mutations compared to wild-type PETase (N233K/R224Q/S121E from prediction and D186H/R280A from scaffold) and shows superior PET-hydrolytic activity relative to both wild-type and engineered alternatives12 between 30 and 50 °C and a range of pH levels. We demonstrate that untreated, postconsumer-PET from 51 different thermoformed products can all be almost completely degraded by FAST-PETase in 1 week. FAST-PETase can also depolymerize untreated, amorphous portions of a commercial water bottle and an entire thermally pretreated water bottle at 50 ºC. Finally, we demonstrate a closed-loop PET recycling process by using FAST-PETase and resynthesizing PET from the recovered monomers. Collectively, our results demonstrate a viable route for enzymatic plastic recycling at the industrial scale.},
issn = {1476-4687},
doi = {10.1038/s41586-022-04599-z},
url = {https://doi.org/10.1038/s41586-022-04599-z},
}

@article{rosenblatt1958,
    author = "Rosenblatt, F.",
    title = "The Perceptron: A Probabilistic Model For Information Storage And Organization in the Brain",
    year = "1958",
    journal = "Psychological Review"
}

@article{backpropagation1,
    author = "Kelley, Henry J.",
    year = "1960",
    title = "Gradient theory of optimal flight paths",
    journal = "ARS Journal. 30 (10): 947-954",
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}


@article{atom-3d,
  author    = {Raphael J. L. Townshend and
               Martin V{\"{o}}gele and
               Patricia Suriana and
               Alexander Derry and
               Alexander Powers and
               Yianni Laloudakis and
               Sidhika Balachandar and
               Brandon M. Anderson and
               Stephan Eismann and
               Risi Kondor and
               Russ B. Altman and
               Ron O. Dror},
  title     = {{ATOM3D:} Tasks On Molecules in Three Dimensions},
  journal   = {CoRR},
  volume    = {abs/2012.04035},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.04035},
  eprinttype = {arXiv},
  eprint    = {2012.04035},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-04035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}