% Suggested LaTeX style template for Masters project report submitted at the
% Department of Computer Science and Technology
%
% Markus Kuhn, May 2022
% (borrowing elements from an earlier template by Steven Hand)

\documentclass[12pt,a4paper]{report}

\usepackage[top=20mm, bottom=20mm, left=25mm, right=25mm]{geometry}
% append option ",openright" after "twoside" if you prefer each chapter
% to start on a recto (odd-numbered) page in a double-sided printout
\input{packages.tex}
\newif\ifsubmission % Boolean flag for distinguishing submitted/final version

% Change the following lines to your own project title, name, college, course
\title{Structure-informed protein engineering using equivariant graph neural networks}
\author{Antonia-Irina Boca}
\date{May 2023}
\newcommand{\candidatenumber}{2485A}
\newcommand{\college}{King's College}
\newcommand{\course}{Computer Science Tripos, Part III}
%\newcommand{\course}{Master of Philosophy in Advanced Computer Science}

% Select which version this is:
% For the (anonymous) submission (without your name or acknowledgements)
% uncomment the following line (or let the makefile do this for you)
\submissiontrue
% For the final version (with your name) leave the above commented.

 \clubpenalty = 10000
 \widowpenalty = 10000
 \displaywidowpenalty = 10000
 
\begin{document}
%TC:ignore

% \printlength\textwidth
\begin{sffamily} % use a sans-serif font for the pro-forma cover sheet

\begin{titlepage}
\makeatletter

% University logo with shield hanging in left margin
\hspace*{-14mm}\includegraphics[width=65mm]{logo-dcst-colour}

\ifsubmission

% submission proforma cover page for blind marking
\begin{Large}
\vspace{20mm}
Research project report title page

\vspace{35mm}
Candidate \candidatenumber

\vspace{42mm}
\textsl{``\@title''}

\end{Large}

\else

% regular cover page
\begin{center}
\Huge
\vspace{\fill}

\@title
\vspace{\fill}

\@author
\vspace{10mm}

\Large
\college
\vspace{\fill}

\@date
\vspace{\fill}

\end{center}

\fi

\vspace{\fill}
\begin{center}
Submitted in partial fulfillment of the requirements for the\\
\course
\end{center}

\makeatother
\end{titlepage}

\newpage

Total page count: \pageref{lastpage}

% calculate number of pages from
% \label{firstcontentpage} to \label{lastcontentpage} inclusive
\makeatletter
\@tempcnta=\getpagerefnumber{lastcontentpage}\relax%
\advance\@tempcnta by -\getpagerefnumber{firstcontentpage}%
\advance\@tempcnta by 1%
\xdef\contentpages{\the\@tempcnta}%
\makeatother

Main chapters (excluding front-matter, references and appendix):
\contentpages~pages
(pp~\pageref{firstcontentpage}--\pageref{lastcontentpage})

Main chapters word count: 11664

Methodology used to generate that word count:

\begin{quote}
\begin{verbatim}
$ ./texcount.pl -inc -total -sum masters-report/report.tex

Total
Sum count: 11673
Words in text: 10450
Words in headers: 231
Words outside text (captions, etc.): 753
Number of headers: 87
Number of floats/tables/figures: 25
Number of math inlines: 201
Number of math displayed: 38
Files: 8
\end{verbatim}
\end{quote}

\end{sffamily}

\vspace{\fill}
\onehalfspacing
\ifsubmission\else\makeatletter
\textbf{\Huge Declaration}
\vspace{40pt}

I, \@author\ of \college, being a candidate for the \course, hereby
declare that this report and the work described in it are my own work,
unaided except as may be specified below, and that the report does not
contain material that has already been used to any substantial extent
for a comparable purpose.

% Add here things like: Figure X is the work of Y, etc.

\bigskip 
\textbf{Signed:} Antonia-Irina Boca

\bigskip
\textbf{Date:} \today
\vspace{\fill}
\makeatother\fi

\chapter*{Abstract}

Computational protein engineering plays a crucial role in advancing scientific research and technological applications in various fields, with pre-trained machine learning models being successful in many protein engineering tasks. 
Most notably, models trained on protein sequences have achieved state-of-the-art performance on protein fitness prediction while models trained on protein structures have been used experimentally to develop proteins with enhanced functions. 

Despite the experimental success of pre-trained structural methods for protein engineering \cite{torng20173d, Lu2022}, several crucial aspects remain unexplored. Firstly, these methods have not been systematically compared with sequence-based approaches using the same datasets. Secondly, their potential to augment assay labelled data, when available, has not been evaluated.

This project initially aimed to evaluate the performance of equivariant graph neural networks (EGNNs) in predicting amino acid residue identity based on local atomic environments, and I report a \textbf{new state-of-the-art} performance on this task. 

I extended the project's scope to utilise these trained models as structure-based models for proposing single-point mutations in proteins. By leveraging the inferred biophysical knowledge derived from pre-training on local atomic environments, these EGNN models can act as unsupervised predictors for the fitness of mutated proteins. I explore systematic approaches to extract candidate mutations based on the models' confidence when dealing with targeted positions. 

I find that structural models based on EGNNs have a competitive performance to sequence-based approaches, while being trained on \textbf{15,909x} fewer of molecules. Additionally, I show how the positional scores generated by these models can be used to augment linear models for protein fitness prediction, effectively operating in a low-data regime: ridge regression models augmented with scores from EGNNs can surpass the performance of pre-trained sequence models with as few as 100 data points.

This project has made significant strides in understanding EGNNs' potential to improve protein design and optimisation, with results indicating the efficacy and potential of structure-based models in addressing key challenges in protein engineering.

\ifsubmission\else
% not included in submission for blind marking:

\chapter*{Acknowledgements}

This project would not have been possible without the wonderful
support of Simon Mathis, whose invaluable guidance and expertise in the field of biology have been instrumental in shaping this dissertation. As a computer scientist with limited knowledge of biology, I was fortunate to have Simon as a mentor that patiently taught me the fundamental concepts and provided me with the necessary tools to navigate this interdisciplinary research project. 

\fi
\cleardoublepage % preserve page numbers after missing acknowledgements

\tableofcontents
\listoffigures
\listoftables
%TC:endignore
\chapter{Introduction}

\label{firstcontentpage} % start page count here
\input{masters-report/chapters/introduction}
\chapter{Background}

\input{masters-report/chapters/background.tex}

\chapter{Related work}

\input{masters-report/chapters/related_work.tex}

\chapter{Design and implementation}

\input{masters-report/chapters/design_implementation.tex}

\chapter{Evaluation}
\label{results}
\input{masters-report/chapters/evaluation}


\chapter{Conclusions}

\input{masters-report/chapters/conclusion.tex}

\label{lastcontentpage} % end page count here
%TC:ignore
\bibliographystyle{unsrtnat}
\bibliography{reference}
% For bibLaTeX users:
% \printbibliography

\clearpage


\appendix
\chapter{Appendix}
\section{Correlation to Tranception ranking}
\label{tranception-correlation}

\begin{table}[!h]
\centering
\caption{Average Spearman rank correlation (for better than wildtype predictions) between Tranception and structure-based models. }
\vskip 0.15in
\begin{tabular}{@{}ccc@{}}
\toprule
Model                  & Ranking    & Correlation to Tranception \\ \midrule
\multirow{2}{*}{EQGAT} & Global     & 0.212                      \\
                       & Positional & 0.146                      \\ \midrule
\multirow{2}{*}{GVP}   & Global     & 0.147                      \\
                       & Positional & 0.073                      \\ \bottomrule
\end{tabular}

\end{table}

\begin{figure}[!h]
    \centering
    \subfigure[\raggedright Spearman's rank correlation between the ranking made by the GVP model and Tranception.]{\includegraphics[width=\textwidth]{masters-report/figures/GVP_tranception_corr.pdf}}
    % \label{fig:gvp-tranception-corr}
    \vspace{1cm}
    \subfigure[\raggedright Spearman's rank correlation between the ranking made by the EQGAT model and Tranception.]{
    \includegraphics[width=\textwidth]{masters-report/figures/EQGAT_tranception_corr.pdf}}
    % \label{fig:eqgat-tranception-corr}
\caption{Correlation between rankings made by Tranception and rankings made by our structure-based models. Missing values indicate that \textbf{fewer than 2} mutations that are better than the wildtype were proposed.}
\end{figure}
\label{lastpage}

\FloatBarrier
\clearpage
\section{Other ablation studies}
\subsection{Experimental structure vs. AlphaFold structures}
\label{appendix-exp-vs-alphafold}
As mentioned in Section \ref{sec:structure-recovery}, we expect the usage of AlphaFold structures to be detrimental to the overall performance of our models. To check whether this is indeed the case, we perform on ablation study on 14 of the 49 sequences we use in this project. These sequences have both a complete experimental structure and an AlphaFold structure, so we can compare the performance of our models using either one or the other. Since the ablation study presented in Section \ref{mutation-discard} makes it clear that mutations at wrongly predicted positions are detrimental to our models, we perform this second ablation study by also discarding mutations for positions that models get wrong.

As presented in Tables \ref{ablation-alphafold-global} and \ref{ablation-alphafold-positional}, we find that there isn't a clear relation between using the AlphaFold structure and a decrease in the better than wildtype Spearman correlation, as it seeems to depend on both the model and the ranking strategy used. However, we notice notice that in 3 out of 4 cases, models rank \textit{worse than wildtype} mutations better when using the AlphaFold structure. 

\begin{table}[!h]
\caption{Model performance when performing \textbf{global} ranking using either AlphaFold or experimental features. Statistics are averages across 14 sequences.}
\label{ablation-alphafold-global}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Structure} & \multicolumn{3}{c}{Spearman's rank correlation}  \\ \cmidrule(l){3-5} 
                       &                            & Average        & Worse than WT  & Better than WT \\ \midrule
EQGAT                  & AlphaFold                  & \textbf{0.311} & \textbf{0.177} & 0.136          \\
EQGAT                  & Experimental               & 0.262          & 0.154          & \textbf{0.157} \\ \midrule
GVP                    & AlphaFold                  & \textbf{0.237} & \textbf{0.211} & \textbf{0.049} \\
GVP                    & Experimental               & 0.202          & 0.128          & $-0.011$         \\ \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[!h]
\caption{Model performance when performing \textbf{positional} ranking using either AlphaFold or experimental features. Statistics are averages across 14 sequences.}
\label{ablation-alphafold-positional}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}

\begin{tabular}{@{}ccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Structure} & \multicolumn{3}{c}{Spearman's rank correlation}  \\ \cmidrule(l){3-5} 
                       &                            & Average        & Worse than WT  & Better than WT \\ \midrule
EQGAT                  & AlphaFold                  & \textbf{0.235} & 0.097          & \textbf{0.149} \\
EQGAT                  & Experimental               & 0.223          & \textbf{0.128} & 0.118          \\ \midrule
GVP                    & AlphaFold                  & \textbf{0.253} & \textbf{0.332} & 0.172          \\
GVP                    & Experimental               & 0.106          & -0.009         & \textbf{0.276} \\ \bottomrule
\end{tabular}

\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Full structure vs. local environment}
\label{appendix-full-vs-local}
One implicit assumption that we make in Section \ref{sec:structure-recovery} is that we input the entire molecule in the GNN, par the masked amino-acid position we wish to predict scores for. This may not necessarily be the best approach, because the models are trained on \textit{samples of local environments}, which contain on average 600 nodes, whereas a full molecular structure can have even 4000 nodes. 

Tables \ref{ablation-local-environment-positional} and \ref{ablation-local-environment-global} show that the EQGAT model benefits from using the entire structure, while the GVP model benefits from using the local environment.
\begin{table}[!h]
\caption{Model performance when performing positional ranking using either local environments or the full molecule. Statistics are averaged across 49 sequences.}

\label{ablation-local-environment-positional}
% \vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{sc}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Structure} & \multicolumn{3}{c}{Spearman's rank correlation}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top 10 \\ precision\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top 10 \\ recall\end{tabular}} \\ \cmidrule(lr){3-5}
                       &                            & Average        & Worse than WT  & Better than WT &                                                                              &                                                                           \\ \midrule
EQGAT                  & Full                       & \textbf{0.223} & \textbf{0.128} & \textbf{0.118} & 0.486                                                                        & \textbf{0.187}                                                            \\
EQGAT                  & Local                      & 0.203          & 0.039          & 0.041          & \textbf{0.516}                                                               & 0.176                                                                     \\ \midrule
GVP                    & Full                       & 0.106          & -0.009         & 0.276          & \textbf{0.462}                                                               & \textbf{0.419}                                                            \\
GVP                    & Local                      & \textbf{0.203} & \textbf{0.104} & \textbf{0.311} & 0.451                                                                        & 0.382                                                                     \\ \bottomrule
\end{tabular}
\end{sc}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[!h]
\caption{Model performance when performing global ranking using either local environments or the full molecule. Statistics are averaged across 49 sequences.}

\label{ablation-local-environment-global}
% \vskip 0.15in
\begin{center}
\begin{footnotesize}
\begin{sc}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Structure} & \multicolumn{3}{c}{Spearman's rank correlation}   & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top 10 \\ precision\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Top 10 \\ recall\end{tabular}} \\ \cmidrule(lr){3-5}
                       &                            & Average        & Worse than WT  & Better than WT  &                                                                              &                                                                           \\ \midrule
EQGAT                  & Full                       & \textbf{0.262} & \textbf{0.154} & \textbf{0.157}  & 0.491                                                                        & \textbf{0.072}                                                            \\
EQGAT                  & Local                      & 0.254          & 0.149          & 0.134           & \textbf{0.491}                                                               & 0.047                                                                     \\ \midrule
GVP                    & Full                       & 0.202          & 0.128          & \textbf{-0.011} & \textbf{0.426}                                                               & 0.100                                                                     \\
GVP                    & Local                      & \textbf{0.216} & 0.233          & \textbf{-0.031} & 0.392                                                                        & \textbf{0.126}                                                            \\ \bottomrule
\end{tabular}
\end{sc}
\end{footnotesize}
\end{center}
\vskip -0.1in
\end{table}

\clearpage


\section{Per-dataset performance breakdown}
\label{appendix:per-dataset-breakdown}
\begin{figure}[!h]
    \centering
    \subfigure[Per-dataset performance of EQGAT.]{
        \includegraphics[width=\textwidth]{masters-report/figures/EQGAT_better_than_WT_per_dataset.pdf}
    }
    \vspace{1cm}
    \subfigure[Per-dataset performance of GVP.]{
        \includegraphics[width=\textwidth]{masters-report/figures/GVP_better_than_WT_per_dataset.pdf}
    }
    \caption{Breakdown of the better than wildtype Spearman rank correlation on our structure-based models, with Tranception as reference. Missing values indicate that \textbf{at most} 1 mutations proposed were better than the wildtype.}
    \label{per-dataset-breakdown}
\end{figure}
\clearpage
\section{Training and mutation generation commands}
Terminal command used to train the GVP model:

\begin{quote}
\begin{verbatim}
python3 ~/rds/hpc-work/partIII-amino-acid-prediction/res_task/main.py
--model gvp 
--data_file ~/rds/hpc-work/split-by-cath-topology/data/ 
--slurm --gpus 1 --num_nodes 1  
--batch_size 64 --n_layers 5 
--data_workers 16 --epochs 40 
\end{verbatim}
\end{quote}
Terminal command used to train the EQGAT model:
\begin{quote}
\begin{verbatim}
python3 ~/rds/hpc-work/partIII-amino-acid-prediction/res_task/main.py 
--model eqgat 
--data_file ~/rds/hpc-work/split-by-cath-topology/data/ 
--slurm --gpus 1 --num_nodes 1  
--batch_size 64 --n_layers 5 
--data_workers 16 --epochs 40 
\end{verbatim}
\end{quote}
Terminal command used to generate mutations with the GVP model (discarding mutations at wrongly predicted positions and using experimental structures when available):
\begin{quote}
\begin{verbatim}
python3 mutations_eval.py --mapper './data/mapping.csv' 
--model_path '~/part3-res-prediction-diss/lf9niuld/checkpoints/
        epoch=40-step=2230113.ckpt' 
--batch_size 2 --model 'gvp' 
--correct_only --out_dir './data'
\end{verbatim}
\end{quote}
Terminal command used to generate mutations with the EQGAT model (discarding mutations at wrongly predicted positions and using experimental structures when available):
\begin{quote}
\begin{verbatim}
python3 mutations_eval.py --mapper './data/mapping.csv' 
--model_path '~/part3-res-prediction-diss/nvqpopw8/checkpoints/
        epoch=39-step=2175720.ckpt'
--batch_size 2 --correct_only 
--model 'eqgat' --out_dir './data'
\end{verbatim}
\end{quote}
\end{document}
