The overall design of this project can be split into three phases:

\begin{enumerate}
    \item Residue identity prediction: in this phase, we train two equivariant GNN models on the RES task, using the ATOM 3D dataset \cite{atom-3d}.
    \item Mutation generation: in this phase, we repurpose the models trained in Phase 1 to generate possible amino-acid mutations for a subset of proteins from the ProteinGym dataset \cite{tranception}; we then evaluate how much better these mutations are than the wildtype (base protein). 
    \item Protein fitness prediction: in the last phase, we use the models trained in Phase 1 to generate position scores for every possible amino-acid in a sequence. We use these scores as features in a ridge regression model in order to predict the fitness of each single-point mutation in the ProteinGym dataset \cite{tranception}.
\end{enumerate}

\section{Residue identity prediction}
The Residue Identity (RES) task focuses on the prediction of amino acid identities within a protein's structural environment. Specifically, our objective is to accurately classify the amino acid present at a given site based on the surrounding atoms. To address this task, we use the ATOM3D dataset \cite{atom-3d}, comprising of atomic environments extracted from non-redundant structures in the Protein Data Bank (PDB). This dataset serves as the foundation for formulating the RES task as a classification problem. 

\subsection{Dataset details}
One of the standard formats for representing protein structure is the Protein Data Bank format (PDB). This text-based format contains information about all of the levels of protein's structure (i.e., primary, secondary, and tertiary). For the tertiary structure, the file describes the 3D coordinates of the atoms in the protein. Each atom record includes information such as the atom name, atom type, residue name, residue number, and the X, Y, and Z coordinates. Table \ref{dataset_stats} provides a summary of the statistics of the full RES ATOM3D dataset.

\paragraph{Local environments.} For each molecule in the dataset, the creators provide samples of local atomic environments. Each sample indicates the indices of the atoms to select from the original PDB file, as well as the target amino-acid.
We use these samples to generate masked local environments to feed into the ML model. Figure \ref{protein_pipeline} illustrates this pipeline conceptually. 

\paragraph{Side-chain masking.} As mentioned in Section \ref{amino-acids}, all amino-acid residues share the same underlying \textit{backbone} structure. Due to this reason, when masking the target amino-acid, the authors of the RES dataset only remove its \textit{side-chain} atoms. The side-chain is always bound to the $\text{C}_{\alpha}$ atom of the residue, so we train the ML model to classify this atom as one of the 20 naturally occurring amino-acids.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/data_pipeline.png}
    \caption{Data generation pipeline for the RES task. The local atomic environments are sampled according to indices provided by the ATOM3D dataset \cite{atom-3d}. The red nodes represent the $\text{C}_{\alpha}$ carbons of their respective amino-acid residues; these are the nodes we are interested in classifying correctly.}
    \label{protein_pipeline}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
    Dataset    & No. molecules & Avg no. samples/molecule & Avg no. atoms/sample \\ \midrule
    Train      & 21072           & 181                       & 622                   \\
    Validation & 962             & 199                       & 612                   \\
    Test       & 3303            & 196                       & 613                   \\ \bottomrule
    \end{tabular}
    \caption{Statistics of the ATOM3D RES dataset.}
    \label{dataset_stats}
\end{table}

\subsection{Training overview}
The training pipeline is summarised in Figure \ref{res-task}. We treat this task as a node classification task, and train the model to predict the most likely side-chain for a target $\text{C}_{\alpha}$ atom given its local environment.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/training-pipeline.png}
    \caption{Diagram of phase 1, illustrating the pipeline used in training the models on the RES task.}
    \label{res-task}
\end{figure}
\paragraph{Graph readout.} More formally, for a given graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with nodes $i,j \in \mathcal{V}$ and edges $(i \rightarrow j) \in \mathcal{E}$ for which we have initial scalar and vector node features $\mathbf{H}\in\mathbb{R}^{|\mathcal{V}|\times n}\times\mathbb{R}^{|\mathcal{V}|\times 3 \times \nu}$, as well as scalar and vector edge features $\mathbf{E}\in\mathbb{R}^{|\mathcal{E}|\times m}\times\mathbb{R}^{|\mathcal{E}|\times 3 \times \eta}$, we first consider a \textit{masked} version of this graph, $\mathcal{G}_t=(\mathcal{V}_t, \mathcal{E}_t)$, for which we have masked all the atoms of the side-chain attached to the node $t$ (representing an $\text{C}_{\alpha}$ atom). 

We then formally define the GNN model $\mathbf{G}_{\theta_1}:\mathbb{R}^{|\mathcal{V}_t|\times n}\times\mathbb{R}^{|\mathcal{V}_t|\times 3 \times \nu}\times\mathbb{R}^{|\mathcal{E}_t|\times m}\times\mathbb{R}^{|\mathcal{E}_t|\times 3 \times \eta}\rightarrow \mathbb{R}^{|\mathcal{V}_t|\times o}$ that takes as input the node and edge features and returns final node features  $\mathbf{H}_{\text{out}}$:
\begin{equation}
    \mathbf{H}_{\text{out}} = \mathbf{G}_{\theta_1}(\mathbf{H}_t, \mathbf{E}_t)
\end{equation}
where $\mathbf{H}_t$ and $\mathbf{E}_t$ represent the node and edge features for all nodes and edges that exist in the masked graph $\mathcal{G}_t$.

Since we are interested in predicting the type of amino-acid corresponding to the masked side-chain of node $t$, we pass its final features $\mathbf{h}_{\text{target}} = [\mathbf{H}_{\text{out}}]_{t}$ through a multi-layer perceptron $\text{MLP}_{\theta_2}:\mathbb{R}^o\rightarrow \mathbb{R}^{20}$ to obtain the final scores associated to each of the 20 naturally occuring amino-acids:
\begin{equation}
    l = \text{MLP}_{\theta_2}(\mathbf{h}_{\text{target}})
\label{mlp-pred}
\end{equation}
Hence, the masking process, the overall GNN model, and the downstream MLP form the node classification function $f_{\gamma}^t:\mathbb{R}^{|\mathcal{V}|\times n}\times\mathbb{R}^{|\mathcal{V}|\times 3 \times \nu}\times\mathbb{R}^{|\mathcal{E}|\times m}\times\mathbb{R}^{|\mathcal{E}|\times 3 \times \eta}\rightarrow \mathbb{R}^{20}$ with learnable parameters $\gamma = \{\theta_1, \theta_2\}$:
\begin{equation}
    f_{\gamma}^t(\mathbf{H}, \mathbf{E}) = \text{MLP}_{\theta_2}([\mathbf{G}_{\theta_1}(\mathbf{H}_t, \mathbf{E}_t)]_t)
\label{full-formalism}
\end{equation}

The \textbf{loss function} of our model quantifies the dissimilarity between the predicted probability distribution associated with scores $l$ obtained in Equation \ref{mlp-pred} and the true probability distribution associated with the target class, also known in the literature as the cross-entropy loss:
\begin{equation}
    \mathcal{L}(l, y) = -\log\frac{\exp({l_y})}{\sum_{c=1}^{20} \exp({l_c})}
\label{logits}
\end{equation}
where $y \in \{1,2,\dots,20\}$ is the true class of target node $t$.
\subsection{The geometric vector perceptron}
\label{the-gvp-math}
The Geometric Vector Perceptron (GVP) is an equivariant learning module first introduced by \citet{gvp1}. It lays the foundation for the GVP-GNN model, an equivariant GNN that has proven to be a successful architecture for many structure-based molecular tasks \cite{gvp1, gvp2}. In this work, we are using the second version of the GVP-GNN architecture, as described in \citet{gvp2}. 

\paragraph{The GVP module.}
The GVP module can learn both scalar-valued and vector-valued functions over geometric vectors and scalars; it can be thought of as generalising the concept of a Multi-Layer Perceptron to vectors, hence its ability to learn geometric properties of molecules. 

Borrowing some notation from \cite{gvp1}, for each node in a graph we can define the tuple $\mathbf{h} = (\mathbf{s}, \mathbf{V})$ of scalar features $\mathbf{s}\in\mathbb{R}^{n}$ and vector features $\mathbf{V}\in \mathbb{R}^{\nu\times 3}$. Then, the GVP computes updated features $\mathbf{h}'=(\mathbf{s}', \mathbf{V}') \in \mathbb{R}^m\times\mathbb{R}^{\mu \times 3}$ as can be seen in Algorithm \ref{gvp-algo}.
\begin{figure}[!h]
    \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/gvp.png}
    \label{fig:image}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.65\textwidth}
    \input{algorithms/gvp}
  \end{minipage}
\caption{\textit{(Left)} An illustration of the data flow of the module; \textit{(Right)} Algorithm showing the computation involved in a GVP module.  Both the pseudocode and the illustration are adapted from \cite{gvp2}.}
\label{gvp-algo}
\end{figure}

\paragraph{The message-passing framework.} The GVP module can be incorporated in the message-passing framework to create a \textit{GVP convolutional layer}. If $\mathbf{h}_{\mathcal{V}}^{(j)}$ are the node features of node $j$ and $\mathbf{h}_{\mathcal{E}}^{(j\rightarrow i)}$ are the edge features of edge $(j \rightarrow i)$, then the module creates the message $\mathbf{m}^{(j\rightarrow i)}$ passed from node $j$ to node $i$ as follows:
\begin{equation}
\mathbf{m}^{(j\rightarrow i)} = g_1\Big(\text{concat}(\mathbf{h}_{\mathcal{V}}^{(j)}, ~~\mathbf{h}_{\mathcal{E}}^{(j \rightarrow i)})\Big) 
\end{equation}
Assuming $k$ is the number of incoming messages, we aggregate them according to:
\begin{equation}
\mathbf{m}_{\mathcal{V}}^{(i)}= \text{LayerNorm}\Big(\mathbf{h}_{\mathcal{V}}^{(i)} + \frac{1}{k}\text{Dropout}\big(\sum_{(j \rightarrow i)\in\mathcal{E}}\mathbf{m}^{(j\rightarrow i)}\big)\Big)
\label{aggregation}
\end{equation}
We follow this with a \textit{feed-forward point-wise} layer to update the node embeddings:
\begin{equation}
    \mathbf{h}_{\mathcal{V}}^{(i)}= \text{LayerNorm}\Big(\mathbf{m}_{\mathcal{V}}^{(i)} + \text{Dropout}\big(g_2(\mathbf{m}_{\mathcal{V}}^{(i)})\big)\Big)
\label{update}
\end{equation}
Functions $g_1$ and $g_2$ are compositions of three and two GVPs, respectively.

\paragraph{Layer normalisation and dropout.} Dropout \cite{dropout} is a regularisation technique that randomly ``drops out'' a portion of the neurons during training, while layer normalisation \cite{layernorm} is a technique that normalises the activations of neurons within a layer to address the problem of internal covariate shift. Both techniques are widely used in practice to avoid overfitting. 

When dealing with scalar features, the equations \ref{aggregation} and \ref{update} use the standard implementations of these two layers. However, when dealing with vector features, \citet{gvp1} modify the Dropout layer to ``drop'' entire 3D vectors. Additionally, they define layer normalisation for vector features to scale the row vectors of $\mathbf{V} =[\mathbf{v}_1, \dots, \mathbf{v}_{\nu}]$, where $\nu$ is the number of vector features per node, such that the root-mean-square norm is equal to 1: 
\begin{equation}
    \mathbf{V} \leftarrow \mathbf{V}/\sqrt{\frac{1}{\nu}||\mathbf{V}||_2^2} ~~\in \mathbb{R}^{\nu\times 3}  
\end{equation}


The final architecture is composed of GVP modules, layer normalisation modules and GVP convolutional layers, as illustrated in Figure \ref{gvp-architecture}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/gvp_architecture_horizontal.png}
    \caption{Overall architecture of the GVP-GNN model.}
    \label{gvp-architecture}
\end{figure}

\paragraph{Initial features.} 
The initial features used on the GVP-GNN architecture are separated into node features $(\mathbf{s}_{\text{node}}, \mathbf{V}_{\text{node}})$ and edge features $(\mathbf{s}_{\text{edge}}, \mathbf{V}_{\text{edge}})$. We obtain the scalar node features by passing its node type $z_{\text{node}}$ through a trainable embedding layer to obtain $\mathbf{s}_{\text{node}} = \text{embed}(z_{\text{node}})$, where $\text{embed}:\mathcal{A}\rightarrow \mathbb{R}^n$. All of the initial node vector features $\mathbf{V}_{\text{node}}$ are set to $\vec{\mathbf{0}}$.

To obtain the scalar edge features $\mathbf{s}_{\text{edge}}$, for each edge from node $i$ to node $j$ we encode the interatomic distance $d_{ji}$ in terms of Gaussian radial basis functions. More formally, for hyperparameter $\epsilon\in\mathbb{R}_{+}$ controlling the smoothness of the basis function, we define the encoding of $d_{ji}$ to be:
\begin{equation}
    \text{GBF}_{\epsilon}(d_{ji}) = e^{-(\epsilon d_{ji})^2}
\end{equation}
We use 16 different $\epsilon$ values, evenly spaced between 0 to 20, rendering the final scalar edge features $\mathbf{s}_{ij} \in \mathbb{R}^{16}$. For the vector embeddings we obtain the normalised edge direction: $\vec{\mathbf{v}}_{ij} = \frac{\mathbf{p}_i - \mathbf{p}_j}{||\mathbf{p}_i - \mathbf{p}_j||_2}$. 

\subsection{The equivariant graph attention network}
\label{eqgat-math}
Attention in neural networks refers to a mechanism that enables the network to focus on specific parts of the input data or relevant information while processing a task. It is inspired by human attention and aims to improve the network's ability to handle long sequences or complex patterns. The most popular attention mechanism to date was introduced by \citet{vaswani2017attention} and lies at the core of the Transformer architecture. 
{\color{red}
The attention mechanism typically involves two components: a query and a set of key-value pairs. The query represents the information the network is currently processing, while the key-value pairs represent the contextual information or memory. The attention mechanism computes the similarity between the query and the keys to obtain \textit{attention weights}, which determine the importance of each value.}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/attention.png}
    \caption{Overall idea of the self-attention mechanism used in \citet{vaswani2017attention}.}
    \label{attention}
\end{figure}

\citet{gat} expanded the attention principle to the realm of GNNs by introducing \text{Graph Attention Networks} (GATs). The attention mechanism proposed by the authors allows the model to focus on different neighbours of a node while aggregating information from its neighbourhood. It assigns attention weights to the neighboring nodes based on their relevance to the target node. The attention weights are learned during the training process and reflect the importance of each neighbour for the target node.

In this work, we are using a version of the GAT network that applies the \textit{self-attention} principle to both scalar and geometric features, allowing the model to learn structural information. \citet{eqgat} propose the \textbf{Equivariant Graph Attention Network} (EQGAT), a model that is equivariant to 3D rotations and translations.

\paragraph{The attention mechanism.} Using a similar notation to the one in Section \ref{the-gvp-math}, for a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, given two nodes $i, j \in \mathcal{V}$ with scalar features $\mathbf{h}_i, \mathbf{h}_j \in \mathbb{R}^{n}$ and edge features $\mathbf{e}_{ji}\in \mathbb{R}^n$, we first compute the \textit{query} and \textit{key} vectors by passing the features through two linear layers:
\begin{align}
    \mathbf{q}_i &= \mathbf{W}_q\mathbf{h}_i + \mathbf{b}_q \in \mathbb{R}^n\\
    \mathbf{k}_j &= \mathbf{W}_k\mathbf{h}_j + \mathbf{b}_k \in \mathbb{R}^n
\end{align}
We then take the element-wise product of the key, query and edge features and apply a linear transformation to it to retrieve a vector of attention coefficients $\mathbf{a}_{ji} \in \mathbb{R}^{n + 2\nu}$:
\begin{align}
    \mathbf{\Tilde{a}} &= \mathbf{q}_i \odot \mathbf{k}_j \odot \mathbf{e}_{ji} \\
    \mathbf{a}_{ji} &= \mathbf{W}_a\mathbf{\Tilde{a}}_{ji} = [\Tilde{\mathbf{\alpha}}_{ji}, \mathbf{\beta}_{ji}, \mathbf{\gamma}_{ji}]
\label{attention-coeffs}
\end{align}
The vector $\Tilde{\mathbf{\alpha}}_{ji} \in \mathbb{R}^{n}$ is then used to compute the final attention coefficients for the \textit{scalar features} as follows:
\begin{equation}
    \mathbf{\alpha}_{ji} = \frac{\sigma(\Tilde{\mathbf{\alpha}}_{ji})}{\sum\limits_{j':(j'\rightarrow i)\in\mathcal{E}}\sigma(\Tilde{\mathbf{\alpha}}_{ji})} \in (0, 1)^{n}
\end{equation}
We then compute the \textit{value} tensors for the attention mechanism using weight matrices $\mathbf{W}_{sv}\in\mathbb{R}^{n\times (n+2\nu)}$ and $\mathbf{W}_{vv}\in\mathbb{R}^{n\times n}$:
\begin{align}
    \label{value-tensors} \mathbf{v}_{s, j} &= [\mathbf{v}_{s_0}, \mathbf{y}_{v_0, j}, \mathbf{y}_{v_1, j}] = \mathbf{W}_{sv}\mathbf{h}_j + \mathbf{b}_{sv}  
    \\ \vec{\mathbf{v}}_{v, j} &= \vec{\mathbf{v}}_j\mathbf{W}_{vv}
\end{align}

Finally, we extend the attention mechanism to the vector features. Given that we have initial vector features $\vec{\mathbf{v}}_i, \vec{\mathbf{v}}_j$ for nodes $i$ and $j$, we create vector attention coefficient $\vec{\mathbf{a}}_{v, ji}$ as follows:
\begin{equation}
    \vec{\mathbf{a}}_{v, ji} = \vec{\mathbf{v}}_j \times \vec{\mathbf{v}}_i + \vec{\mathbf{v}}_{v, j}
\end{equation}

\paragraph{Message construction.}
Once we have constructed all necessary features for the attention mechanism, we can compute the incoming messages for the message-passing framework. First, the scalar message from node $j$ to node $i$ is computed as:
\begin{equation}
    \mathbf{m}_{s, ji} = \mathbf{\alpha}_{ji}\odot\mathbf{v}_{s_0, j}
\end{equation}
Next, we compute two types of vector messages from node $j$ to node $i$ using the coefficients computed in Equation \ref{attention-coeffs} and the value tensors computed in Equation \ref{value-tensors}:
\begin{align}
    \mathbf{m}_{v_0, ji} &= \mathbf{\beta}_{ji}\odot \mathbf{y}_{v_0, j}\\
    \mathbf{m}_{v_1, ji} &= \mathbf{\gamma}_{ji}\odot \mathbf{y}_{v_1, j}    
\end{align}
We use these vector messages to construct \textit{equivariant interactions} between nodes $j$ and $i$ by using their relative position $\vec{p}_{ji} = \vec{p}_i - \vec{p}_j$ in 3D space. More formally, we obtain the normalised position $\vec{p}_{e,ij} = \frac{\vec{p}_{ij}}{||\vec{p}_{ij}||_2}$ and combine this with vector message $\mathbf{m}_{v_0,ji}\in\mathbb{R}^n$ as:
\begin{equation}
    \vec{\mathbf{y}}_{0, ji} = \vec{p}_{e,ij}\otimes\mathbf{m}_{v_0,ji} = \vec{p}_{e,ij}\mathbf{m}_{v_0, ji}^{\top} \in \mathbb{R}^{3\times\nu}
\label{position-cross-product}
\end{equation}
Finally, we create type-1 features:
\begin{equation}
    \vec{\mathbf{y}}_{1, ji} = (\mathbf{1}\otimes\mathbf{m}_{v_1, ji})\odot \vec{\mathbf{a}}_{v, ji}\text{, where }\mathbf{1}\in\mathbb{R}^3
\end{equation}
\paragraph{Message aggregation.}
The final aggregated message that we form for node $i$ sums the messages received from each of its neighbours on the scalar and vector channels, respectively:
\begin{align}
    \mathbf{m}_{s, i} &= \sum_{j\in\mathcal{N}(i)}\mathbf{m}_{s, ji}\\
    \mathbf{m}_{v, i} &= \sum_{j\in\mathcal{N}(i)}(\vec{\mathbf{y}}_{0, ji} + \vec{\mathbf{y}}_{1, ji}) \\
    \mathbf{m}_i &= (\mathbf{m}_{s, i}, \mathbf{m}_{v, i})
\end{align}

\paragraph{Update.} The last step within a message-passing step is the update. For the update, the authors first sum together the updated state $\mathbf{m}_i$ and the previous state $\mathbf{x}_i = (\mathbf{h}_i, \vec{\mathbf{v}}_i)$:
\begin{equation}
    \mathbf{\Tilde{x}}_i = \mathbf{x}_i + \mathbf{m}_i
\end{equation}
Notice how in this way we combine information about the node's previous state $\mathbf{x}_i\in\mathbb{R}^n\times\mathbb{R}^{3\times\nu}$ with the newly created message $\mathbf{m}_i$. The last step in this layer is to perform a pointwise update using \textit{gated equivariant non-linearities}. More formally, given the state $\mathbf{\Tilde{x}}_i = (\mathbf{s}_i, \vec{\mathbf{v}}_i)$, we apply two linear transformations $\mathbf{W}_{u_1}$ and $\mathbf{W}_{u_2}$ to the vector features $\vec{\mathbf{v}}_i$:
\begin{align}
    \vec{\mathbf{u}}_1 &= \mathbf{W}_{u_1} \cdot \vec{\mathbf{v}}_i \\
    \vec{\mathbf{u}}_2 &= \mathbf{W}_{u_2} \cdot \vec{\mathbf{v}}_i 
\end{align}
We allow information flow from the vector dimension to the scalar dimension by concatenating the norm of $\vec{\mathbf{u}}_1$, namely $\mathbf{n} = ||\vec{\mathbf{u}}_1||_2 \in \mathbb{R}^{\nu}$ to $\mathbf{s}_i$ and passing it through a multilayer perceptron with a SiLU nonlinearity. We obtain three embeddings, $\mathbf{u}_{s_1}, \mathbf{u}_{s_2} \in \mathbb{R}^{n}$ and $\mathbf{u}_v \in \mathbf{R}^\nu$, that we will be using in the final update operation:
\begin{equation}
    [\mathbf{u}_{s_1}, \mathbf{u}_{s_2}, \mathbf{u}_v] = \text{MLP}(\text{concat}(\mathbf{s}_i, \mathbf{n}))
\end{equation}
We can now obtain the updated scalar and vector features:
\begin{align}
    \mathbf{s}_i' &= \mathbf{u}_{s_0} + \mathbf{n}^2 \odot \mathbf{u}_{s_1} \\
    \mathbf{\vec{v}}_i' &= \mathbf{u}_v \odot \vec{\mathbf{u}}_2
\end{align}
A diagram of the overall EQGAT architecture can be found in Figure \ref{eqgat}. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/eqgat_diagram.png}
    \caption{(a) The overall message-passing architecture of the EQGAT. (b) The \textit{update} step, allowing information flow from the vector features to the scalar features. (c) The \textit{message construction} step, which creates scalar and vector messages. (d) The \textit{feature-attention} module. Image taken from \cite{eqgat}. }
    \label{eqgat}
\end{figure}

\paragraph{Initial features.}
The initial scalar features used in the model are embeddings created from the atom type of each node, while the initial vector features are set to $\vec{\mathbf{0}}$. Note that after one message-passing layer equivariant vector features are created through the cross-product in Equation \ref{position-cross-product}, which combines the positions of each node with the value tensors created from scalar features. 

Additionally, the edge embedding $\mathbf{e}_{ij}$ encodes the distance between node $i$ and node $j$. We embed the interatomic distance $d_{ij} = ||p_{ij}||_2 \in \mathbb{R}$ using the Bessel radial basis function (RBF), formally defined as:
\begin{equation}
    \text{RBF}_k(d_{ji}) = \sqrt{\frac{2}{c}\frac{\sin(\frac{k\pi}{c}d_{ji})}{d_{ji}}}
\end{equation}
for distance cutoff $c$ and $k = 1, \dots, K$. The reason for encoding the distance using RBFs is that these functions are non-zero only within a specified radius. This property enables them to capture local interactions between atoms while minimising the influence of distant nodes, with parameter $k$ controlling the oscillatory behaviour of the function. Using these encodings instead of the plain Euclidean distance allows the model to learn how to treat different length scales in separate manners. 

We concatenate $\text{RBF}_k(d_{ji})$ for all $k = 1,\dots,K$ to obtain an initial edge features $\mathbf{e}^{\text{RBF}}_{ij}$. We pass these through a trainable linear layer and obtain:
\begin{equation}
    \Tilde{\mathbf{e}}_{ji} = \mathbf{W}_e\mathbf{e}^{\text{RBF}}_{ij} + \mathbf{b}_e
\end{equation}
To obtain the final edge embedding, we combine $\Tilde{\mathbf{e}}_{ji}$ with a cosine-cutoff function $\kappa:\mathbb{R}_{+}\mapsto [0,1]$, defined as:
\begin{equation}
    \kappa(x) = \frac{1}{2}\Big(\cos (\frac{\pi x}{c}) + 1\Big)\cdot \mathbbm{1}[d_{ji}\leq c]
\end{equation}
This is done to remove the discontinuities of the $\text{RBF}_k$ function when $d_{ji}\rightarrow c$ (the distance cutoff). Formally, the final edge embedding $\mathbf{e}_{ji}$ becomes:
\begin{equation}
    \mathbf{e}_{ji} = \kappa(d_{ji})\cdot \Tilde{\mathbf{e}}_{ji}
\end{equation}

\section{Mutation generation}
\label{sec:mutation-generation}
This section describes the strategy used to generate mutations using the two trained GNN models described in Sections \ref{the-gvp-math} and \ref{eqgat-math}. These models are trained to predict the most likely amino-acid given a local atomic environment. Based on these capabilities, we can repurpose them to the task of mutation generation by masking out each amino-acid in a structure \textit{in turn} and passing the masked structure through the model. We then use the scores $l \in \mathbb{R}^{20}$ defined in Equation \ref{logits} to rank the most likely amino-acids for each position. Figure \ref{mutation-generation} illustrates this approach visually. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/mutation_generation_final.png}
    \caption{Pipeline of phase 2.}
    \label{mutation-generation}
\end{figure}


\subsection{Structure recovery}

The ProteinGym substitutions dataset \cite{tranception} contains 87 molecular sequences. For each of these sequences, a number of experimentally tested mutations are scored according to their \textit{fitness}. While the dataset also includes the fitness of sequences that have been mutated at multiple positions, the scope of this project is limited to \textit{single-point} mutations. 

\paragraph{Homomers and oligomers.} Many real-life proteins fold together into protein complexes, also known as \textit{biological assemblies}.
These assemblies can be composed of multiple copies of the same protein (also known as homomeric assemblies or homo-oligomers) or different protein molecules (also known as heteromeric assemblies or hetero-oligomers).
Heteromeric assemblies can also contain other types of molecules, such as  nucleic acids or lipids.
When inducing a mutation into a protein that is part of a protein assembly, it is necessary to replicate it across all identical chains in the complex. Due to this contraint, we reduce the scope of this project to only deal with sequences whose structures are either \textit{homomers} or \textit{homo-oligomers}. 

The ProteinGym dataset does not provide a mapping between sequences and structures, so the first part of the pipeline illustrated in Figure \ref{mutation-generation} involves the recovery of structures from the Protein Data Bank \cite{rcsb_pdb}. We query the PDB through their programmatic API for the biological assembly associated with a sequence. When multiple assemblies are available for the same sequence, we choose a random one. 

These assemblies are determined experimentally using X–ray crystallography, a method that involves exposing a crystallised sample of a molecule to x-rays in order to determine the positions of individual atoms. Some of these experimental assemblies have two major challenges. Firstly, the molecule may be bound to a ligand – usually a smaller molecule that is attached to the main protein and helps it stabilise. X-ray crystallography will reveal both the positions of the atoms in the main protein and the atoms in the ligand. To solve this, we apply a post-processing step that cleans the assembly of any undesired ligands or water molecules; the cleaning code is inspired by an approach taken in the AlphaFold \cite{alphafold} codebase.

The second major challenge is that some proteins contain inherently mobile chains that cannot be crystallised. When this is the case, the biological assembly will have incomplete structure that lacks parts of the chain associated with the sequence. When this happens, we discard the biological assembly and instead use the AlphaFold homomer prediction if it exists. 

\paragraph{AlphaFold structures.} As mentioned in the Introduction, AlphaFold is a machine learning model developed by DeepMind \cite{alphafold} that is capable of predicting with high accuracy the structure into which a protein sequence will fold. The second version of the model, AlphaFold 2, is considered one of the major scientific breakthroughs of the 21st century and is now routinely used to predict protein structures by researchers. 

One drawback of AlphaFold 2, however, is that it can only predict the structure of homomers. This means that it cannot be easily used to build up hetero-oligomers, and for homo-oligomeric assemblies it will only predict the homomer from which the complex is formed, as exemplified in Figure \ref{alpha-fold-homomer}. The major drawback of using a monomer instead of the entire biological assembly to generate mutations is that amino-acids lying on the outskirts of a molecule may be wrongly flagged as viable for mutations, when in reality they should bind to other copies of themselves. A more thorough analysis of the impact of using AlphaFold structures instead of biological assemblies will be discussed in Chapter \ref{results}, where an ablation study will compare the performance difference when using predicted monomers instead of the full biological assembly.

\begin{figure}
\centering
    \subfigure[The 6EZM molecule, commonly known as Baker's Yeast. It is formed of 24 copies of the same monomer.]{
        \includegraphics[scale=0.25]{masters-report/figures/6ezm-yeast.png}
        \label{homomer}
    }
    \hspace{0.3in}
    \subfigure[The monomer predicted by AlphaFold for the 6EZM molecule.]{
        \includegraphics[scale=0.25]{masters-report/figures/alpha-fold-pred.png}
    }
    \caption{Example of (a) a mono 24-mer and  (b) the AlphaFold prediction for sequence of the mono 24-mer.}
    \label{alpha-fold-homomer}
\end{figure}

The overall structure processing pipeline is presented in Figure \ref{filtering-steps}, which highlights all the points during the post-processing of structures where a design decision is made. 
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{masters-report/figures/sequence_recovery.png}
    \caption{The structure processing pipeline.}
    \label{filtering-steps}
\end{figure}

\subsection{Positional masking}
\label{positional-masking}
The main idea behind our mutation generation strategy is that for every sequence for which we have obtained a clean structure according to the filtering pipeline presented in Figure \ref{filtering-steps} we follow the steps below to determine all possible single-point amino-acid mutations in the sequence:
\begin{enumerate}
    \item Firstly, we mask the amino-acid residue present at a target position in the structure; 
    \item Secondly,we pass the masked structure through the GNN model; 
    \item Thirdly, we recover the score corresponding to the probability of each of the 20 naturally-occurring amino-acids to be in the masked position. 
\end{enumerate}
We repeat this process for all positions in the sequence. 
Intuitively, a higher probability (i.e., model confidence) for a \textit{residue} indicates that the model believes it would be a good match for that position; conversely, an overall lower confidence at a target \textit{position} indicates the position is amenable to mutations.

\subsection{Mutation ranking}
Once we have the scores associated with each of amino-acid mutation for every position in a sequence, we propose two strategies for ranking these mutations: global and positional. Formally, given a wildtype sequence $x_1x_2\dots x_n$ of length $n$ with $x_i \in \mathcal{A} = \{1,2,\dots,20\}$ representing the index of amino-acid $i$, associated to a atomic graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, the process described in \ref{positional-masking} builds a scoring function of the positions  $S:\{1,2,\dots,n\}\times\mathcal{A}\rightarrow \mathbb{R}$ that we define by extending the formalism in Equation \ref{full-formalism}:
\begin{equation}
    S(i, a) = [f_{\gamma}^{g(i)}(\mathbf{H}, \mathbf{E})]_a
\label{scoring-function}
\end{equation}
Where $g:\{1,2,\dots,n\}\rightarrow\{1,2,\dots,|\mathcal{V}|\}$ is a mapping function from positions to the index of the node representing the central $\text{C}_{\alpha}$ of the residue present at said position. 

Equation \ref{scoring-function} essentially represents the score of amino-acid $a$ for target position $i$, associated with node $g(i)$ in the atomic graph. Note that the true amino-acid at the same position is denoted by $x_i$. 
We are now ready to formally define the two ranking strategies. 

\paragraph{Global ranking.} The first approach to ranking the scores of our mutations is to sort them in descending order of their scores, regardless of the position they occupy. If we denote the single-point mutation to amino-acid $a$ at position $i$ by $\mathbf{m}_{i}^a$, then $\forall i,j\text{ and }\forall a,b\in \mathcal{A} \text{ s.t. } a \neq x_i \text{ and } b \neq x_j$, we say that:
\begin{equation}
    \mathbf{m}_{i}^a\text{ is better than }\mathbf{m}_{j}^b \iff S(i, a) > S(j, b)
\label{global-ranking}
\end{equation}

\paragraph{Positional ranking.} The second approach follows when we prioritise the positions we want to mutate instead of the amino-acids we mutate to. Instead of quantifying mutations by their global score compared to all other mutations, we rank mutations by the confidence of the model in predicting the wildtype residue at each position. Intuitively, if the model confidence in the wildtype residue $x_i$ at position $i$ is low, this is an indication that the position itself may be prone accept mutations. Formally, this can be quantified as:
\begin{equation}
\begin{aligned}
&\mathbf{m}_{i}^a\text{ is better than }\mathbf{m}_{j}^b \\ 
\iff &\Big(S(i, x_i) < S(j, x_j)\Big) \lor \Big(S(i, x_i) = S(j, x_j) \land S(i, a) > S(j, b)\Big)
\end{aligned}
\label{positional-ranking}
\end{equation}
Equation \ref{positional-ranking} ranks mutations first by their positions; that is, the less confident the model is in a wildtype residue at a certain position, the higher any mutation on that position will rank. In the case of ties (when comparing mutations to different amino-acids at the same position), we compare the actual mutation score. Additionally, we only keep \textit{the top 3} mutations for each position. 

\section{Protein fitness prediction}
As a second extension to the original scope of the project, we propose a straightforward fitness prediction model that can learn from an already existing dataset of single-point mutations to predict the fitness of a sequences. Following an approach similar to the one used for mutation generation in Section \ref{sec:mutation-generation}, we obtain the scores of all single-point amino-acid mutations for every position in every wildtype sequence part of the ProteinGym substitutions dataset \cite{tranception}. We use these scores to augment a baseline regression model to perform protein fitness prediction. A summary of the pipeline of this phase is illustrated in \ref{fitness-prediction}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{masters-report/figures/protein_fitness_prediction.png}
    \caption{Pipeline of phase 3.}
    \label{fitness-prediction}
\end{figure}

\subsection{Baseline model} 
We start off with a simple ridge regression model that uses a similar approach to the one proposed by \citet{chloe-hsu}. Formally, for a mutated sequence $s_i = a_1a_2\dots a_n$, we consider its features for the regression to be:
\begin{equation}
    \mathbf{x}_i = [~\text{embed}(a_1) ~||~ \text{embed}(a_2) ~||~ \dots ~||~\text{embed}(a_n)~] ~\in~\mathbb{R}^{n \times d} 
\label{baseline-regression}
\end{equation}
wherd $d\in\mathbb{N}$ is the dimension of the encoding function $\text{embed}:\mathcal{A}\rightarrow \mathbb{R}^d$. The value to predict, $y_i$, represents the fitness of the sequence.

We evaluate two types of encodings in this project:
\begin{enumerate}
    \item The \textbf{one-hot} encoding;
    \item The \textbf{AAIndex} embeddings. The AAIndex database \cite{aa-index} is a database of numeric indicators that represent a wide range of physicochemical and biochemical characteristics of amino-acids. We create embeddings from these indices by parsing all known characteristics and performing principle component analysis on them to generate 19-dimensional feature vectors. 
\end{enumerate}

\subsection{Augmented models} 
We augment these simple regression models by adding the score of the single-point mutation present in the sequence $s_i$ to the feature vector $\mathbf{x}_i$. Formally, if sequence $s_i = a_1a_2\dots a_n$ has single-point mutation $\mathbf{m}_i^a$, then the feature vector becomes:
\begin{equation}
    \mathbf{x}_i' = [~\text{embed}(a_1) ~||~ \text{embed}(a_2) ~||~ \dots ~||~ \text{embed}(a_n) ~||~ {\color{blue} S(i, a)}~] ~\in~\mathbb{R}^{n\times d + 1}
\label{augmented-regression}
\end{equation}
Where $S(i, a)$ is the same scoring function as in Equation \ref{scoring-function}. 

Note that this way of creating feature vectors takes into consideration only single-point mutations, hence we add a single new scalar to the feature vector $\mathbf{x}_i$. When dealing with the wildtype sequence, we concatenate either $1$ or $0$ to the original feature vector $\mathbf{x}_i$, depending on the dataset.

\subsection{Training data}
The regression models defined in Equations \ref{baseline-regression} and \ref{augmented-regression} are trained on each wildtype sequence from the ProteinGym dataset separately. These models provide a quick and easy way of predicting the fitness of new mutations given a number of already existing datapoints, hence performing well in a low-data regime. As it will be shown in Chapter \ref{results}, we train the regressions on increasingly larger number of single-point mutated samples for each sequence in the ProteinGym dataset and show how they can outperform state-of-the-art unsupervised methods.  
